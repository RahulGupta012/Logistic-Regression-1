{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde3af7e-7eaa-45ef-ba75-e6da85d3359d",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93789605-7fec-4078-9728-96380af16359",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31260b0a-274d-41fe-ad31-1027d09eeaea",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5cd62-08b2-4e1d-a031-4d96960d1b32",
   "metadata": {},
   "source": [
    "There are some differences between liner and logistic regression ;\n",
    "\n",
    "Liner Regression are generally used for  continues dependent features but Logistic Regression are used in categorical dependent features as binary or multiple values. However we can also predict the best fit line for categorical features by using the liner Regression . But in case of outliers the predictions are very low precentge of rightness and in other cases , for binary values , some times prediction goes beyond 0 and 1..That is why liner regression can not we considered for such type of categorical features. While in the logistic regression the intial equation is similar to the liner regression as h(theta) = theta_0 + theta(1)*X(1) : but inlogistic regression , there is a sigmoid function which works for squacing the prediction value in x and y coordinate , till the range of 0 to 1 (binary formate). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a8a84-b680-4502-8315-fad760d14526",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2f93b-d3b8-410b-9ca4-da81d07f0d5e",
   "metadata": {},
   "source": [
    "cost function for logistic regression known as the log loss also. The cost function of logistic regression somehow similer to the liner regression but in the logistic regression there is a additional function , which is called as 'Sigmoid Activation Function'. This function is basicaly squacing the predicted values in between 0 to 1. \n",
    "\n",
    "J(w) = -1/m * (sum(y*log(y_hat) + (1-y)*log(1-y_hat)))\n",
    "\n",
    "where:\n",
    "\n",
    "J(w) is the cost function\n",
    "w is the set of weights used in the logistic regression model\n",
    "m is the number of training examples\n",
    "y is the actual binary outcome (0 or 1) of the training example\n",
    "y_hat is the predicted probability of the binary outcome (0 or 1) of the training example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293551ea-fbae-46bd-aaf1-e3e6fbc926b9",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492f3c8-ff41-4526-a20f-e8cfe2499b96",
   "metadata": {},
   "source": [
    "The concept of regularization is using for reducing the overfitting of the model as well as for feature selection. It added a penality term to the cost function of logistic regression.Overfitting is a problem of the model, which occurs when a model becomes too complex and fits the training data too closely, and gives very geranalized predictions for new data.\n",
    " There are three types of regulazations:\n",
    " \n",
    " 1. L1 Regulazation (lasso Regression) : usually used for reducing overfitting\n",
    "\n",
    " 2. l2 Regulazation (Ridge Regression) : usually used for feature selection\n",
    " \n",
    " 3. Elastic Net Regression : Comibination of both (lasso and ridge)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8213cb-b19d-4663-ba7d-de34c5c905a1",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49b5dd-e41d-46dd-8464-3856c76857f1",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation that helps evaluate the performance of binary classification models, such as logistic regression models. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various classification thresholds. It basically shows a graph that shows how well a binary classifier model performs at different threshold values. \n",
    "\n",
    "The ROC curve plots two parameters: \n",
    "- True Positive Rate\n",
    "- False Positive Rate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ef841-0981-4420-8bba-bb2083653797",
   "metadata": {},
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbaab5-761b-40a7-ba60-2c3875f536dd",
   "metadata": {},
   "source": [
    "As we discussed above, that there are some techniques, by which we can improve the performance of our model. Feature selction is important as in our data set there are many feature for training the model, we should always try to minimize the number of features so that our prediction will be precise and accurate. There are many feature which has less or more impact on the dependent feature.We need to identify these features with respect of their correlation to the output features. For slected these feature we aquired such techniques:\n",
    "\n",
    "Regulaztion:  L1 and L2 regulazation applying the penality to the cost function of logistic regression by which he coefficients of less important features tend become to zero. and feature slelction would be exicuted.\n",
    "\n",
    "Univariate feature selection: It is a statical opretion by which we find the correlation of the feature and give the preference to the more correlated feature.\n",
    "\n",
    "Recursive feature elimination: In this technique we remove those feature who gives very low impact on output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c8f16-f9c3-4e38-b1dd-0057b18ff630",
   "metadata": {},
   "source": [
    "# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3d0e7-f4b6-4eea-9417-420a1ec19e14",
   "metadata": {},
   "source": [
    "Imbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class.\n",
    "\n",
    "Upsampling : it involves increasing the number of instances in the minority class to balance the class representation.\n",
    "\n",
    "Downsampling :educes the sample rate of a signal while keeping its essential information\n",
    "\n",
    "SMOTE ; In this technique the synthetic samples are generated for the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d8ceb-b3c6-4b79-aa21-5317924f3fb9",
   "metadata": {},
   "source": [
    "# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3e328-ca5b-4ac6-8da9-9d5e7d7a166c",
   "metadata": {},
   "source": [
    "There are some issues may arise on implementing logistic regression:\n",
    "    \n",
    "1. Overfiiting : Overfitting is a common issue which mey be arising while implementing the logistic regression. This challenge can be countered by the help of regulazations : l2 and l1 as we discussed above.\n",
    "\n",
    "2. Outliars : It is a also a common issue , which decrese our model accuracy , to overcome this we can use boxplot garphs for visulaze the outliars and for removing them, we can use z score and IQR techniques. We can also do standardization the values as well.\n",
    "\n",
    "3. Imbalanced Dataset : In machine learning classification where there are a disproportionate ratio of observations in each class.We can counter this problem by smote and sampling techniques\n",
    "\n",
    "4. Multicollinearity: It is also a big problem. when we use logistic regression we find the Multicollinearity: in the independent features . Which also is not good for model performance. We can counter this isuue by feature selection as we should remove those multiple independent feature which are highly correleated to each other and keep only one independent feature amoung them(if two , one of them).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbeab2b-0d67-4f38-8d22-21ea6d9c207e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
